{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install --upgrade pip\n","!pip install mujoco\n","!pip install gymnasium[mujoco]\n","!pip install --upgrade tensorflow\n","!pip install highway-env"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T20:48:03.284989Z","iopub.status.busy":"2023-12-07T20:48:03.284209Z","iopub.status.idle":"2023-12-07T20:48:04.488929Z","shell.execute_reply":"2023-12-07T20:48:04.487633Z","shell.execute_reply.started":"2023-12-07T20:48:03.284943Z"},"trusted":true},"outputs":[],"source":["from highway_env.envs.common.action import ContinuousAction"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T20:48:13.051827Z","iopub.status.busy":"2023-12-07T20:48:13.051203Z","iopub.status.idle":"2023-12-07T20:48:19.316195Z","shell.execute_reply":"2023-12-07T20:48:19.314879Z","shell.execute_reply.started":"2023-12-07T20:48:13.051791Z"},"id":"Ppnd2LraPTk8","trusted":true},"outputs":[],"source":["import warnings\n","\n","# Disable all warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import tensorflow as tf\n","import gymnasium as gym\n","import random\n","from collections import deque\n","import numpy as np\n","import pdb\n","import datetime\n","import wandb\n","import os\n","import pdb\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras import Input\n","from tensorflow.keras.layers import Dense, BatchNormalization, Lambda, Dropout, Conv2D, Flatten\n","from tensorflow.keras.losses import mean_squared_error\n","from tensorflow.keras.optimizers import Adam, SGD\n","from tensorflow.keras.initializers import glorot_normal, RandomUniform\n","from tensorflow.keras.regularizers import l2\n","\n","tf.keras.utils.disable_interactive_logging()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T20:48:19.319145Z","iopub.status.busy":"2023-12-07T20:48:19.318655Z","iopub.status.idle":"2023-12-07T20:48:19.326414Z","shell.execute_reply":"2023-12-07T20:48:19.324257Z","shell.execute_reply.started":"2023-12-07T20:48:19.319100Z"},"trusted":true},"outputs":[],"source":["ENV_NAME = \"parking-v0\"\n","KER_REG  = l2(0.01)\n","OBSERVATION_SHAPE = (128, 128, 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T20:48:25.401384Z","iopub.status.busy":"2023-12-07T20:48:25.400625Z","iopub.status.idle":"2023-12-07T20:49:00.031677Z","shell.execute_reply":"2023-12-07T20:49:00.030633Z","shell.execute_reply.started":"2023-12-07T20:48:25.401337Z"},"executionInfo":{"elapsed":3158,"status":"ok","timestamp":1695478520447,"user":{"displayName":"Moussa NIANG","userId":"01108136908997265991"},"user_tz":0},"id":"7tnrw54n4xNt","outputId":"5a6df643-2052-4e76-e8f2-841b89af531c","trusted":true},"outputs":[],"source":["wandb.login(key=\"YOUR_WANDB_API_KEY_HERE\")\n","run = wandb.init(project=\"Deep-Learning-Project\", name=f\"{ENV_NAME} DDPG\", resume=None)"]},{"cell_type":"markdown","metadata":{"id":"pOkcqNq2Uhkf"},"source":["## Replay Buffer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T20:50:16.580009Z","iopub.status.busy":"2023-12-07T20:50:16.579514Z","iopub.status.idle":"2023-12-07T20:50:16.592456Z","shell.execute_reply":"2023-12-07T20:50:16.591063Z","shell.execute_reply.started":"2023-12-07T20:50:16.579961Z"},"id":"9r2edZ4rUoSV","trusted":true},"outputs":[],"source":["class ReplayBuffer:\n","    def __init__(self, buffer_size):\n","        self.buffer_size = buffer_size\n","        self.buffer = deque(maxlen=buffer_size)\n","\n","    def add(self, state, action, reward, next_state, done):\n","        experience = (state, action, reward, next_state, done)\n","        self.buffer.append(experience)\n","\n","    def sample(self, batch_size):\n","        batch = random.sample(self.buffer, batch_size)\n","        states, actions, rewards, next_states, dones = map(np.asarray, zip(*batch))\n","        return states, actions, rewards, next_states, dones\n","\n","    def __len__(self):\n","        return len(self.buffer)\n","\n","    def is_full(self):\n","        return len(self.buffer) == self.buffer_size\n","\n","    def clear(self):\n","        self.buffer.clear()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T20:50:17.253558Z","iopub.status.busy":"2023-12-07T20:50:17.252819Z","iopub.status.idle":"2023-12-07T20:50:17.260643Z","shell.execute_reply":"2023-12-07T20:50:17.258749Z","shell.execute_reply.started":"2023-12-07T20:50:17.253524Z"},"id":"nyFA61vpx3Tr","trusted":true},"outputs":[],"source":["def custom_initializer(fan_in):\n","    limit = 1.0 / np.sqrt(fan_in)\n","    return tf.keras.initializers.RandomUniform(minval=-limit, maxval=limit)"]},{"cell_type":"markdown","metadata":{"id":"DJEQ9ldsWTJ_"},"source":["\n","## Actor Network"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T20:50:18.229235Z","iopub.status.busy":"2023-12-07T20:50:18.227980Z","iopub.status.idle":"2023-12-07T20:50:18.249354Z","shell.execute_reply":"2023-12-07T20:50:18.247178Z","shell.execute_reply.started":"2023-12-07T20:50:18.229183Z"},"id":"LlGxh5PmWU83","trusted":true},"outputs":[],"source":["class Actor:\n","\n","  def __init__(self, actor_lr, observation_shape, action_shape, action_bound):\n","    self.observation_shape = observation_shape\n","    self.action_shape = list(action_shape)[0]\n","    self.opt = Adam(learning_rate=actor_lr)\n","    self.action_bound = action_bound\n","    self.model = self.create_model()\n","\n","  def create_model(self):\n","\n","    # The actor is a deep neural network taking as input a state and predict the action for that state\n","\n","    # Input shape = observation space shape; Output shape = action space shape\n","    model = Sequential()\n","\n","    model.add(Input(shape=self.observation_shape))\n","    model.add(BatchNormalization())\n","    \n","    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer=custom_initializer(27)))\n","    model.add(BatchNormalization())\n","    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer=custom_initializer(288)))\n","    model.add(BatchNormalization())\n","    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer=custom_initializer(288)))\n","    model.add(BatchNormalization())\n","    \n","    model.add(Flatten())\n","    \n","    model.add(Dense(200, activation='relu', kernel_initializer=custom_initializer(288)))\n","    model.add(BatchNormalization())\n","    model.add(Dense(200, activation='relu', kernel_initializer=custom_initializer(200)))\n","    model.add(BatchNormalization())\n","\n","    model.add(Dense(self.action_shape, activation='tanh', kernel_initializer=RandomUniform(minval=-0.0003, maxval=0.0003)))\n","    \n","    return model\n","\n","\n","  def get_weights(self):\n","    return self.model.get_weights()\n","\n","\n","  def set_weights(self, weights):\n","    return self.model.set_weights(weights)\n","\n","\n","  def predict(self, states):\n","    return self.model(states, training=False)\n","\n","\n","  def train(self, states, critic):\n","    with tf.GradientTape() as tape:\n","      predicted_actions = self.model(states, training=True)\n","      actor_loss = -tf.math.reduce_mean(critic.predict(states, predicted_actions))\n","      gradient = tape.gradient(actor_loss, self.model.trainable_variables)\n","    self.opt.apply_gradients(zip(gradient, self.model.trainable_variables))\n","    return actor_loss"]},{"cell_type":"markdown","metadata":{"id":"QUgAjjeTjhtX"},"source":["## Critic Network"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T20:50:19.194898Z","iopub.status.busy":"2023-12-07T20:50:19.194478Z","iopub.status.idle":"2023-12-07T20:50:19.219746Z","shell.execute_reply":"2023-12-07T20:50:19.218299Z","shell.execute_reply.started":"2023-12-07T20:50:19.194861Z"},"id":"oG7bya4ljpU5","trusted":true},"outputs":[],"source":["class Critic:\n","\n","  def __init__(self, critic_lr, observation_shape, action_shape):\n","    self.observation_shape = observation_shape\n","    self.action_shape = action_shape\n","    self.input_shape = tuple(x+y for x, y in zip(observation_shape, action_shape))\n","    self.opt = Adam(learning_rate=critic_lr)\n","    self.model = self.create_model()\n","\n","  def create_model(self):\n","    \n","    # State\n","    state_input = Input(self.observation_shape)\n","    normalized_input = BatchNormalization()(state_input)\n","    \n","    s0 = Conv2D(32, (3, 3), activation='relu', kernel_initializer=custom_initializer(27), kernel_regularizer=KER_REG)(normalized_input)\n","    s0 = BatchNormalization()(s0)\n","    s1 = Conv2D(32, (3, 3), activation='relu', kernel_initializer=custom_initializer(288), kernel_regularizer=KER_REG)(s0)\n","    s1 = BatchNormalization()(s1)\n","    s2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer=custom_initializer(288), kernel_regularizer=KER_REG)(s1)\n","    s2 = BatchNormalization()(s2)\n","    \n","    flatten = Flatten()(s2)\n","    \n","    fc = Dense(100, activation='relu', kernel_initializer=custom_initializer(150), kernel_regularizer=KER_REG)(flatten)\n","    fc = BatchNormalization()(fc)\n","\n","    # Action\n","    action_input = Input(self.action_shape)\n","    action_input = BatchNormalization()(action_input)\n","    \n","    a1 = Dense(100, activation='relu', kernel_initializer=custom_initializer(100), kernel_regularizer=KER_REG)(action_input)\n","    a1 = BatchNormalization()(a1)\n","    \n","    fc1 = tf.concat([fc, a1], axis=-1)\n","    fc1 = BatchNormalization()(fc1)\n","    \n","    fc2 = Dense(200, activation='relu', kernel_initializer=custom_initializer(100), kernel_regularizer=KER_REG)(fc1)\n","    \n","    output = Dense(1, activation='linear', kernel_initializer=RandomUniform(minval=-0.0003, maxval=0.0003))(fc2)\n","\n","    return tf.keras.Model([state_input, action_input], output)\n","\n","\n","  def get_weights(self):\n","    return self.model.get_weights()\n","\n","\n","  def set_weights(self, weights):\n","    return self.model.set_weights(weights)\n","\n","\n","  def q_grads(self, states, actions):\n","    with tf.GradientTape() as tape:\n","        tape.watch(actions)\n","        q_values = self.model([states, actions], training=False)\n","    grads = tape.gradient(q_values, actions)\n","    return grads\n","\n","    \n","  def predict(self, states, actions):\n","    input_data = [states, actions]\n","    return self.model(input_data, training=False)\n","\n","\n","  def train(self, states, actions, target_critic):      \n","    with tf.GradientTape() as tape:\n","\n","      # Concatenate the states and action to form input for critic network\n","      policy_critic = tf.squeeze(self.model([states, actions], training=True))\n","\n","      # Compute MSE loss between target_critic and current policy critic\n","      loss = mean_squared_error(target_critic, policy_critic)\n","      gradient = tape.gradient(loss, self.model.trainable_variables)\n","    \n","    self.opt.apply_gradients(zip(gradient, self.model.trainable_variables))\n","#     pdb.set_trace()\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T20:50:19.819187Z","iopub.status.busy":"2023-12-07T20:50:19.818755Z","iopub.status.idle":"2023-12-07T20:50:19.884014Z","shell.execute_reply":"2023-12-07T20:50:19.882691Z","shell.execute_reply.started":"2023-12-07T20:50:19.819152Z"},"id":"XPVh0oYK6XdJ","trusted":true},"outputs":[],"source":["CONFIG = {\n","    \"action\": {\n","        \"type\": \"ContinuousAction\",\n","    },\n","    \"observation\": {\n","       \"type\": \"GrayscaleObservation\",\n","       \"observation_shape\": (128, 128),\n","       \"stack_size\": 3,\n","       \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion\n","    },\n","    \"render_agent\": False,\n","}\n","\n","env = gym.make(ENV_NAME)\n","env.configure(CONFIG)\n","continuous_env = ContinuousAction(ENV_NAME)\n","\n","class Agent:\n","\n","  def __init__(self, env_name, actor_lr, critic_lr, batch_size, buffer_size, tau, gamma, train_start, start_steps):\n","\n","    self.env = gym.make(env_name)\n","    self.env.configure(CONFIG)\n","    \n","    self.observation_shape = OBSERVATION_SHAPE\n","    self.action_shape = continuous_env.space().shape\n","    \n","    self.tau = tau\n","    self.gamma = gamma\n","    self.buffer_size = buffer_size\n","    self.batch_size = batch_size\n","    self.train_start = train_start\n","    self.start_steps = start_steps\n","    \n","    self.continue_training = False    \n","\n","    # Initialize Replay Buffer\n","    self.replay_buffer = ReplayBuffer(buffer_size)\n","\n","    bound = 1\n","    \n","    # Initialize actor and critic networks\n","    self.actor = Actor(actor_lr, self.observation_shape, self.action_shape, bound)\n","    self.critic = Critic(critic_lr, self.observation_shape, self.action_shape)\n","\n","    # Initialize target actor and target critic networks\n","    self.target_actor = Actor(actor_lr, self.observation_shape, self.action_shape, bound)\n","    self.target_critic = Critic(critic_lr, self.observation_shape, self.action_shape)\n","    self.target_actor.set_weights(self.actor.get_weights())\n","    self.target_critic.set_weights(self.critic.get_weights())\n","    \n","    # Create checkpoint for each object\n","    self.actor_cpt = tf.train.Checkpoint(model=self.actor.model)\n","    self.critic_cpt = tf.train.Checkpoint(model=self.critic.model)\n","    self.t_actor_cpt = tf.train.Checkpoint(model=self.target_actor.model)\n","    self.t_critic_cpt = tf.train.Checkpoint(model=self.target_critic.model)\n","\n","    # Paths for pretrained weights\n","    actor_cpt_dir = f\"./{env_name}/actor\"\n","    critic_cpt_dir = f\"./{env_name}/critic\"\n","    target_actor_cpt_dir = f\"./{env_name}/target_actor\"\n","    target_critic_cpt_dir = f\"./{env_name}/target_critic\"\n","    \n","    self.actor_cpt_manager = tf.train.CheckpointManager(self.actor_cpt, actor_cpt_dir, max_to_keep=5)\n","    self.critic_cpt_manager = tf.train.CheckpointManager(self.critic_cpt, critic_cpt_dir, max_to_keep=5)\n","    self.t_actor_cpt_manager = tf.train.CheckpointManager(self.t_actor_cpt, target_actor_cpt_dir, max_to_keep=5)\n","    self.t_critic_cpt_manager = tf.train.CheckpointManager(self.t_critic_cpt, target_critic_cpt_dir, max_to_keep=5)\n","\n","    # Load pretrained weights if any\n","    self.init_weights()\n","    \n","    \n","  # Define a function to generate checkpoint filenames with a timestamp\n","  def checkpoint_name(self, ckpt_dir):\n","    return os.path.join(ckpt_dir, \"ckpt\")\n","\n","    \n","  def restore(self, ckpt, manager):\n","    if(manager.latest_checkpoint):\n","        print(\"Restoring from latest checkpoint...\")\n","        ckpt.restore(manager.latest_checkpoint)\n","        self.continue_training = True\n","    else:\n","        print(\"Starting from scratch...\")\n","    \n","    \n","  def init_weights(self):\n","    self.restore(self.actor_cpt, self.actor_cpt_manager)\n","    self.restore(self.critic_cpt, self.critic_cpt_manager)\n","    self.restore(self.t_actor_cpt, self.t_actor_cpt_manager)\n","    self.restore(self.t_critic_cpt, self.t_critic_cpt_manager)\n","    \n","        \n","  def save_weights(self):\n","    self.actor_cpt_manager.save()\n","    self.critic_cpt_manager.save()\n","    self.t_actor_cpt_manager.save()\n","    self.t_critic_cpt_manager.save()\n","    \n","\n","  def update_target(self):\n","    # theta = tau * theta + (1-tau) * theta\n","    theta_actor = self.actor.get_weights()\n","    theta_critic = self.critic.get_weights()\n","\n","    theta_target_actor = self.target_actor.get_weights()\n","    theta_target_critic = self.target_critic.get_weights()\n","\n","    for i in range(len(theta_target_actor)):\n","      theta_target_actor[i] = self.tau * theta_actor[i] + (1 - self.tau) * theta_target_actor[i]\n","\n","    for i in range(len(theta_target_critic)):\n","      theta_target_critic[i] = self.tau * theta_critic[i] + (1 - self.tau) * theta_target_critic[i]\n","\n","    self.target_actor.set_weights(theta_target_actor)\n","    self.target_critic.set_weights(theta_target_critic)\n","    \n","    \n","  def compute_target_critic_value(self, next_states, rewards, dones, target_actor, target_critic):\n","\n","    # Action resulting from target policy\n","    predicted_actions = target_actor.predict(next_states)\n","\n","    # Target critic value\n","    target_critic_prediction = tf.squeeze(target_critic.predict(next_states, predicted_actions))\n","        \n","    y = rewards + self.gamma * (1-dones) * target_critic_prediction\n","\n","    return y\n","\n","\n","  def ou_noise(self, x, rho=0.15, mu=0, sigma=0.2, dt=0.001, dim=1):\n","    return x + rho * (mu-x) * dt + sigma * np.sqrt(dt) * np.random.randn(dim)\n","\n","\n","  def explore(self, state, bg_noise):\n","    bound = 1\n","\n","    # Get exploration noise\n","    noise = self.ou_noise(bg_noise, dim=self.action_shape[0])\n","\n","    # Predict action for current state\n","    action = self.actor.model(np.array([state]), training=False)[0]\n","    action = np.clip(action + noise, -bound, bound)\n","    \n","    # Return sum of action + noise\n","    return action, noise\n","\n","\n","  def replay(self):\n","    states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n","    \n","    # Convert to tensor\n","    states = tf.convert_to_tensor(states, dtype=tf.float32)\n","    actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n","    rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n","    next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n","\n","    # Compute target critic\n","    y = self.compute_target_critic_value(next_states, rewards, dones, self.target_actor, self.target_critic)\n","\n","    # Update Critic\n","    critic_loss = self.critic.train(states, actions, y)\n","    \n","    # Update Actor\n","    actor_loss = self.actor.train(states, self.critic)\n","\n","    # Update Target Actor and Critic\n","    self.update_target()\n","    \n","    return actor_loss, critic_loss\n","    \n","\n","  def train(self):\n","    global_steps = 0\n","    while(global_steps < 1e6):\n","        \n","      # Receive initial observation state\n","      state = self.env.reset(seed=42)[0]\n","      state = np.transpose(state, axes=(1, 2, 0))\n","\n","      episode_reward, done, truncated = 0, False, False\n","      steps = 0\n","    \n","      bg_noise = np.zeros(self.action_shape[0])\n","\n","      while not (done or truncated):\n","        \n","        # Take action according to the current policy and exploration noise\n","        if(global_steps < self.start_steps and not self.continue_training):\n","          # Uniform-random action selection, before running real policy. Helps exploration.\n","          action = self.env.action_space.sample()\n","        else:\n","          action, bg_noise = self.explore(state, bg_noise)\n","\n","        # Execute action a and observe reward and transition\n","        next_state, reward, done, truncated, info = self.env.step(action)\n","        next_state = np.transpose(next_state, axes=(1, 2, 0))\n","        \n","        # Add the (state, action, reward, next_state) to the replay buffer\n","        self.replay_buffer.add(state, action, reward, next_state, done)\n","\n","        # Update current state\n","        state = next_state\n","    \n","        # Update episodic reward\n","        episode_reward += reward\n","        \n","        global_steps += 1\n","        steps += 1\n","        \n","        run.log({\"Reward\": reward})\n","\n","        if((global_steps > self.train_start) and (steps % 5 == 0)):\n","          actor_loss, critic_loss = self.replay()\n","          run.log({\"Actor Loss\": actor_loss, \"Critic Loss\": critic_loss})\n","\n","        if(global_steps > self.train_start and steps % 20 == 0):\n","          print(\"Saving models...\")\n","          self.save_weights()\n","          print(\"Done !\")\n","    \n","      # Log Episodic Reward to WandB\n","      run.log({\"Episode length\": steps, \"Episode Reward\": episode_reward})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T20:50:20.425891Z","iopub.status.busy":"2023-12-07T20:50:20.425021Z","iopub.status.idle":"2023-12-07T20:50:20.436107Z","shell.execute_reply":"2023-12-07T20:50:20.435001Z","shell.execute_reply.started":"2023-12-07T20:50:20.425844Z"},"id":"bbiq14EyS4dp","trusted":true},"outputs":[],"source":["def train_agent(env_name=ENV_NAME, actor_lr=0.0001, critic_lr=0.001,\n","                batch_size=16, buffer_size=1000000, gamma=0.99, tau=0.001, \n","                train_start=1000, start_steps=1000):\n","    try:\n","        with tf.device('/GPU:0'):\n","            agent = Agent(env_name, actor_lr, critic_lr, batch_size, buffer_size, tau, gamma, train_start, start_steps)\n","            agent.train()\n","    except KeyboardInterrupt:\n","        print(\"Terminating training process...\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_agent()"]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30559,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
